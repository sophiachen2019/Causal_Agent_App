Traceback (most recent call last):
  File "/Users/sophiachen/.gemini/antigravity/playground/twilight-kepler/Causal_Agent_SaaS/test_cate_hte_export.py", line 40, in <module>
    exec(generated_code)
    ~~~~^^^^^^^^^^^^^^^^
  File "<string>", line 86, in <module>
  File "<string>", line 84, in estimate_causal_effect
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/dowhy/causal_model.py", line 359, in estimate_effect
    return estimate_effect(
        self._data,
    ...<9 lines>...
        method_params,
    )
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/dowhy/causal_estimator.py", line 753, in estimate_effect
    estimator.fit(
    ~~~~~~~~~~~~~^
        data=data,
        ^^^^^^^^^^
        effect_modifier_names=effect_modifiers,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **method_params["fit_params"] if "fit_params" in method_params else {},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/dowhy/causal_estimators/econml.py", line 190, in fit
    self.estimator.fit(**estimator_data_args, **kwargs)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/econml/_cate_estimator.py", line 134, in call
    m(self, Y, T, *args, **kwargs)
    ~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/econml/metalearners/_metalearners.py", line 235, in fit
    Y, T, X, _ = check_inputs(Y, T, X, multi_output_T=False,
                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                              force_all_finite_X='allow-nan' if 'X' in self._gen_allowed_missing_vars() else True)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/econml/utilities.py", line 524, in check_inputs
    X, T = check_X_y(X, T, multi_output=multi_output_T, y_numeric=True,
           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                     **_get_ensure_finite_arg(force_all_finite_X))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/sklearn/utils/validation.py", line 1370, in check_X_y
    X = check_array(
        X,
    ...<12 lines>...
        input_name="X",
    )
  File "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/sklearn/utils/validation.py", line 1130, in check_array
    raise ValueError(
    ...<3 lines>...
    )
ValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.
Generating script for S-Learner with HTE...
Executing generated script...
Data Simulated Successfully
Effect Identified
Estimating effect using Meta-Learner: S-Learner...

FAILURE: Generated script execution failed:

--- Generated Code ---
import pandas as pd
import numpy as np
import dowhy
from dowhy import CausalModel
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV
from sklearn.preprocessing import StandardScaler
from econml.dml import LinearDML, CausalForestDML
from econml.metalearners import SLearner, TLearner
import matplotlib.pyplot as plt
import statsmodels.api as sm

# --- 1. Load Data ---

def simulate_data(n_samples=1000):
    np.random.seed(42)
    customer_segment = np.random.binomial(1, 0.3, n_samples)
    historical_usage = np.random.normal(50, 15, n_samples) + (customer_segment * 20)
    marketing_nudge = np.random.binomial(1, 0.5, n_samples)
    quarter = np.random.binomial(1, 0.5, n_samples)
    prob_adoption = 1 / (1 + np.exp(-( -2 + 0.5 * customer_segment + 0.05 * historical_usage + 1.5 * marketing_nudge)))
    feature_adoption = np.random.binomial(1, prob_adoption, n_samples)
    account_value = (200 + 500 * feature_adoption + 1000 * customer_segment + 10 * historical_usage + 50 * quarter + np.random.normal(0, 50, n_samples))
    
    df = pd.DataFrame({
        'Customer_Segment': customer_segment,
        'Historical_Usage': historical_usage,
        'Marketing_Nudge': marketing_nudge,
        'Quarter': quarter,
        'Feature_Adoption': feature_adoption,
        'Account_Value': account_value
    })

    # Enforce Data Types
    df['Customer_Segment'] = df['Customer_Segment'].astype(int)
    df['Historical_Usage'] = df['Historical_Usage'].astype(float)
    df['Marketing_Nudge'] = df['Marketing_Nudge'].astype(int)
    df['Quarter'] = df['Quarter'].astype(int)
    df['Feature_Adoption'] = df['Feature_Adoption'].astype(int)
    df['Account_Value'] = df['Account_Value'].astype(float)
    return df

df = simulate_data()
print("Data Simulated Successfully")

# --- 2. Data Preprocessing ---

# --- 3. Causal Model ---
# --- 3. Causal Model ---

# Encoding Categorical Treatment
df['Treatment_Encoded'] = np.nan
df.loc[df['Feature_Adoption'] == '0', 'Treatment_Encoded'] = 0
df.loc[df['Feature_Adoption'] == '1', 'Treatment_Encoded'] = 1
df = df.dropna(subset=['Treatment_Encoded'])
treatment = 'Treatment_Encoded'

outcome = 'Account_Value'
confounders = ['Customer_Segment', 'Historical_Usage']
instrument = None
effect_modifiers = confounders # Using confounders as effect modifiers for heterogeneity

model = CausalModel(
    data=df,
    treatment=treatment,
    outcome=outcome,
    common_causes=confounders,
    instruments=instrument,
    effect_modifiers=effect_modifiers
)

# --- 4. Identify Effect ---
identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
print("Effect Identified")

# --- 5. Estimate Effect ---
estimation_method = "Meta-Learner: S-Learner"
print(f"Estimating effect using {estimation_method}...")

def estimate_causal_effect(model, identified_estimand, test_significance=True):
    estimate = None
    method_name = 'backdoor.econml.metalearners.SLearner'
    init_params = {"overall_model": RandomForestRegressor(random_state=42)}
    estimate = model.estimate_effect(identified_estimand, method_name=method_name, method_params=dict(init_params=init_params, fit_params=dict()))
    return estimate
estimate = estimate_causal_effect(model, identified_estimand)
print(f'Average Treatment Effect (ATE): {estimate.value}')

# --- 6. Refutation ---
print('Running Refutation...')
refute_results = model.refute_estimate(
    identified_estimand,
    estimate,
    method_name='random_common_cause'
)
print(refute_results)

# --- 7. Bootstrapping ---
print(f'Running 10 bootstrap iterations...')
bootstrap_estimates = []
for i in range(10):
    try:
        df_resampled = df.sample(frac=1, replace=True, random_state=i)
        model_boot = CausalModel(
            data=df_resampled,
            treatment='Feature_Adoption',
            outcome='Account_Value',
            common_causes=['Customer_Segment', 'Historical_Usage'],
            instruments=instrument,
            effect_modifiers=['Customer_Segment', 'Historical_Usage']
        )
        identified_estimand_boot = model_boot.identify_effect(proceed_when_unidentifiable=True)
        est_boot = estimate_causal_effect(model_boot, identified_estimand_boot, test_significance=False)
        bootstrap_estimates.append(est_boot.value)
    except Exception:
        pass

    if bootstrap_estimates:
        se = np.std(bootstrap_estimates)
        ci = (np.percentile(bootstrap_estimates, 2.5), np.percentile(bootstrap_estimates, 97.5))
        print(f'Standard Error (SE): {se:.2f}')
        print(f'95% Confidence Interval: [{ci[0]:.2f}, {ci[1]:.2f}]')
    else:
        print('Bootstrapping failed.')

# --- 8. Heterogeneity Analysis ---
print('\nRunning Heterogeneity Analysis for all features...')
hte_results = []
features_to_analyze = ['Customer_Segment', 'Historical_Usage']
for feature in features_to_analyze:
    try:
        # Universal HTE: Regress ITE on Feature
        # Calculate ITE first
        X_test = df[['Customer_Segment', 'Historical_Usage']]
        try:
            if hasattr(estimate.estimator, 'effect'):
                ite = estimate.estimator.effect(X_test)
            elif hasattr(estimate, 'estimator_instance') and hasattr(estimate.estimator_instance, 'effect'):
                ite = estimate.estimator_instance.effect(X_test)
            else:
                ite = None
                print('Warning: Could not extract ITEs from estimator.')
        except Exception as e:
            ite = None
            print(f'Error calculating ITE: {e}')
        
        if ite is not None:
            df_ite = df.copy()
            df_ite['ITE'] = ite.flatten()
            
            X_feat = sm.add_constant(df_ite[feature])
            y_feat = df_ite['ITE']
            model_feat = sm.OLS(y_feat, X_feat).fit()
            coef = model_feat.params[feature]
            pval = model_feat.pvalues[feature]
        else:
            raise ValueError('ITE not found for CATE method.')
        hte_results.append({
            'Feature': feature,
            'Interaction Coefficient': coef,
            'P-value': pval,
            'Significant': 'Yes' if pval < 0.05 else 'No'
        })
    except Exception as e:
        print(f'Skipping {feature} due to error: {e}')
        continue

if hte_results:
    hte_df = pd.DataFrame(hte_results).sort_values('P-value')
    print('\n--- Heterogeneity Analysis Results ---')
    print(hte_df)
else:
    print('No heterogeneity results computed.')

